{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics - Protein subcellular location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from  sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingRegressor\n",
    "from sklearn.linear_model import LogisticRegression,RandomizedLogisticRegression\n",
    "from sklearn.metrics import f1_score,confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load data and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_pipeline(*files):\n",
    "    #p = re.compile(\"(\\w+\\|\\w+)\\|(\\w+\\s[0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\'\\[\\]\\+]+)OS=([0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\']+)GN=([0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\']+)PE=([0-9])+\\s[SV=]+([0-9])|(\\w+\\|\\w+)\\|(\\w+\\s[0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\'\\[\\]]+)OS=([0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\']+)PE=([0-9])+\\s[SV=]+([0-9])\")\n",
    "    p=re.compile(\"\\|\\w+\\s(.+)OS=([0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\']+)(?:\\sGN|\\sPE)\")\n",
    "    data_features = []\n",
    "    data_labels = []\n",
    "    sequence = ''\n",
    "    list_meta=[]\n",
    "    for file in files:\n",
    "        label = os.path.splitext(file)[0]\n",
    "        f = open(file, \"r\")\n",
    "        dict_meta = defaultdict(float)\n",
    "        first_line = f.readline()\n",
    "        meta_info = p.search(first_line)\n",
    "        try:\n",
    "            dict_meta[\"organism\"] = meta_info.group(2)\n",
    "            dict_meta[\"protein\"] = meta_info.group(1)\n",
    "            dict_meta[\"class\"] = label\n",
    "        except:\n",
    "            print(first_line)\n",
    "        list_meta.append(dict_meta)\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if line[0] != '>':\n",
    "                sequence += line\n",
    "            else:\n",
    "                dict_meta[\"sequence\"] = sequence\n",
    "                dict_meta = defaultdict(float)\n",
    "                meta_info = p.search(line)\n",
    "                try:\n",
    "                    dict_meta[\"organism\"] = meta_info.group(2)\n",
    "                    dict_meta[\"protein\"] = meta_info.group(1)\n",
    "                    dict_meta[\"class\"] = label\n",
    "                except:\n",
    "                    print(line)\n",
    "                list_meta.append(dict_meta)\n",
    "                data_features.append(sequence)\n",
    "                data_labels.append(label)\n",
    "                sequence = ''\n",
    "        #Last input\n",
    "        list_meta[-1][\"sequence\"] = sequence\n",
    "        data_features.append(sequence)\n",
    "        data_labels.append(label)\n",
    "        sequence = ''\n",
    "\n",
    "\n",
    "\n",
    "    return data_features, data_labels,list_meta\n",
    "\n",
    "dic_properties = {\n",
    "    'small' : ['A','G','C','S','P','N','C','T','D'],\n",
    "    'tiny' : ['A','G','C','S'],\n",
    "    'polar' : ['K','H','R','D','E','Q','N','S','C','T','Y','W'],\n",
    "    'charged' : ['K','H','R','D','E'],\n",
    "    'positive' : ['K','H','R'],\n",
    "    'negative' :  ['D','E'],\n",
    "    'hidrophobic' : ['F','Y','W','H','I','L','V','A','G','C','M','K','T'],\n",
    "    'aromatic' : ['F','Y','W','H'],\n",
    "    'aliphatic' : ['I','L','V']\n",
    "    \n",
    "}\n",
    "\n",
    "def feat_extract(sequences):\n",
    "    list_dict_feat = []\n",
    "    for sequence in sequences:\n",
    "        \n",
    "        protein = ProteinAnalysis(sequence)\n",
    "        sequence_feat = defaultdict(float)\n",
    "        sequence_len = len(sequence)\n",
    "\n",
    "        sequence_feat[\"sequence_length\"] = sequence_len        \n",
    "        sequence_feat[\"aromaticty\"] = protein.aromaticity()\n",
    "        sequence_feat[\"isoeletric_point\"] = protein.isoelectric_point()\n",
    "        #sequence_feat[\"flexibility\"] = protein.flexibility()\n",
    "        if ('X' not in sequence) and ('O' not in sequence) and ('U' not in sequence) and ('B' not in sequence):\n",
    "            sequence_feat[\"molecular_weight\"] = protein.molecular_weight()\n",
    "        for letter in sequence:\n",
    "            sequence_feat[\"relative_fre_{}\".format(letter)] += 1/sequence_len\n",
    "            for property in dic_properties:\n",
    "                if letter in dic_properties[property]:\n",
    "                    sequence_feat['freq_{}'.format(property)] += 1\n",
    "        for letter in sequence[0:50]:    \n",
    "            sequence_feat[\"relative_fre_start{}\".format(letter)] += 1/50\n",
    "        for letter in sequence[-51:-1]:    \n",
    "            sequence_feat[\"relative_fre_end{}\".format(letter)] += 1/50\n",
    "        list_dict_feat.append(sequence_feat)\n",
    "    return list_dict_feat\n",
    "\n",
    "label_encoder = preprocessing.LabelBinarizer()\n",
    "vectorizer = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(x,y):\n",
    "    \n",
    "    labels_enc = label_encoder.fit_transform(y)\n",
    "    features_enc = vectorizer.fit_transform(feat_extract(x))\n",
    "    \n",
    "    \n",
    "    model = RandomForestClassifier(class_weight='balanced',n_estimators=15)\n",
    "    #clf = BaggingRegressor(model, n_estimators=45, max_samples=0.1, random_state=25)\n",
    "    model = xgboost.XGBClassifier(\n",
    "                 learning_rate =0.1,\n",
    "                 n_estimators=1000,\n",
    "                 max_depth=5,\n",
    "                 min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "    \n",
    "    #model = SVC(class_weight='balanced', probability=True)\n",
    "    #model = LogisticRegression(class_weight='balanced')\n",
    "    #model = RandomizedLogisticRegression()\n",
    "    model.fit(features_enc, y)\n",
    "    #clf.fit(features_enc,y)\n",
    "    return model\n",
    "\n",
    "def validate(x,model):\n",
    "    \n",
    "    \n",
    "    features = vectorizer.transform(feat_extract(x))\n",
    "    predicts = model.predict_proba(features)  \n",
    "    predicts_label = np.argmax(predicts,1)\n",
    "    #labels_predicted = label_encoder.inverse_transform(predicts_label)\n",
    "    #label_and_confidence = list(zip(labels_predicted,np.amax(predicts,1)))\n",
    "\n",
    "    return labels_predicted#,np.amax(predicts,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(ip_dim=80):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000,input_dim=ip_dim,activation=\"tanh\",init='uniform'))\n",
    "    model.add(Dense(4,activation=\"softmax\",init='uniform'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00001, momentum=0.8, decay=0.0, nesterov=False), metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "def train_nn(x,y):\n",
    "    labels_enc = label_encoder.fit_transform(y)\n",
    "    features_enc = vectorizer.fit_transform(feat_extract(x))\n",
    "    print(labels_enc)\n",
    "    print(features_enc.shape)\n",
    "    #features_enc = np.array([[0.3,0.1,0.9],[0.45,0.7,0.1],[0.7,0.7,0.1],[0.3,0.9,0.1], [1,0,0],[0.11,4,2],[0,0,5]])\n",
    "    #labels_enc = np.array([[0,0,1],[0,1,0],[1,0,0],[0,1,0], [1,0,0],[0,1,0],[0,0,1]])\n",
    "    model = create_model(features_enc.shape[1])\n",
    "    model.fit(features_enc, labels_enc, nb_epoch=1000, batch_size=1,verbose=2)\n",
    "    return model\n",
    "\n",
    "def validate_nn(x,y,model):\n",
    "    labels_enc = label_encoder.fit_transform(y)\n",
    "    features_enc = vectorizer.fit_transform(feat_extract(x))\n",
    "    loss_and_metrics = model.evaluate(features_enc,labels_enc, batch_size=32)\n",
    "    return loss_and_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis(=1) out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-a393ced4d6eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#nn = train_nn(train_x,train_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#print(validate_nn(val_x,val_y,nn))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-cbbcb5e5a3d7>\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(x, model)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mpredicts_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mlabels_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicts_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mlabel_and_confidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_predicted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Joao Reis\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, Y, threshold)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0my_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_inverse_binarize_multiclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             y_inv = _inverse_binarize_thresholding(Y, self.y_type_,\n",
      "\u001b[1;32mC:\\Users\\Joao Reis\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_inverse_binarize_multiclass\u001b[1;34m(y, classes)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_i_argmax\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"clip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axis(=1) out of bounds"
     ]
    }
   ],
   "source": [
    "data_sequence, data_labels, meta_info = preprocess_pipeline('cyto.fasta', 'mito.fasta','nucleus.fasta','secreted.fasta')\n",
    "train_x, val_x, train_y, val_y = train_test_split(data_sequence,data_labels,test_size=0.3,random_state=3)\n",
    "\n",
    "rf = train(train_x,train_y)\n",
    "\n",
    "pred_y = validate(val_x,rf)\n",
    "#nn = train_nn(train_x,train_y)\n",
    "#print(validate_nn(val_x,val_y,nn))\n",
    "#print(meta_info)\n",
    "\n",
    "#df = pd.DataFrame(meta_info)\n",
    "\n",
    "#print(df)\n",
    "#with open('ola.csv','w') as f:\n",
    "#    df.to_csv(f)\n",
    "\n",
    "#df.groupby(\"organism\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-180-d12f5d78179b>:46 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "----- Epoch 0 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 2487.76556396\n",
      "0.32501936483346244\n",
      "----- Epoch 1 -----\n",
      "['secreted' 'secreted' 'secreted' ..., 'secreted' 'secreted' 'secreted']\n",
      " Train loss: 467.908295525\n",
      "0.1721146398140976\n",
      "----- Epoch 2 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 314.901582506\n",
      "0.14624322230828815\n",
      "----- Epoch 3 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 187.901176876\n",
      "0.14004647560030983\n",
      "----- Epoch 4 -----\n",
      "['nucleus' 'nucleus' 'nucleus' ..., 'mito' 'nucleus' 'nucleus']\n",
      " Train loss: 92.5146748225\n",
      "0.3024012393493416\n",
      "----- Epoch 5 -----\n",
      "['secreted' 'secreted' 'secreted' ..., 'secreted' 'secreted' 'secreted']\n",
      " Train loss: 119.812685384\n",
      "0.17273431448489543\n",
      "----- Epoch 6 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 144.084669325\n",
      "0.3301316808675445\n",
      "----- Epoch 7 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 150.27456326\n",
      "0.2021688613477924\n",
      "----- Epoch 8 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 128.559962591\n",
      "0.330286599535244\n",
      "----- Epoch 9 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 100.35709805\n",
      "0.330286599535244\n",
      "----- Epoch 10 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 116.730247921\n",
      "0.33044151820294343\n",
      "----- Epoch 11 -----\n",
      "['secreted' 'secreted' 'secreted' ..., 'secreted' 'secreted' 'secreted']\n",
      " Train loss: 129.552166833\n",
      "0.17010069713400464\n",
      "----- Epoch 12 -----\n",
      "['secreted' 'secreted' 'secreted' ..., 'secreted' 'secreted' 'secreted']\n",
      " Train loss: 144.847316318\n",
      "0.17335398915569325\n",
      "----- Epoch 13 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 79.2761590746\n",
      "0.14748257164988382\n",
      "----- Epoch 14 -----\n",
      "['nucleus' 'nucleus' 'nucleus' ..., 'nucleus' 'nucleus' 'nucleus']\n",
      " Train loss: 89.7488689423\n",
      "0.3659178931061193\n",
      "----- Epoch 15 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 80.3289881812\n",
      "0.32997676219984506\n",
      "----- Epoch 16 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 108.423499637\n",
      "0.330286599535244\n",
      "----- Epoch 17 -----\n",
      "['secreted' 'secreted' 'secreted' ..., 'secreted' 'secreted' 'secreted']\n",
      " Train loss: 112.708689796\n",
      "0.1731990704879938\n",
      "----- Epoch 18 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 71.2344618903\n",
      "0.33044151820294343\n",
      "----- Epoch 19 -----\n",
      "['cyto' 'nucleus' 'nucleus' ..., 'nucleus' 'cyto' 'nucleus']\n",
      " Train loss: 82.4714020623\n",
      "0.3575522850503486\n",
      "----- Epoch 20 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 164.066488478\n",
      "0.14779240898528273\n",
      "----- Epoch 21 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 159.143788232\n",
      "0.14763749031758328\n",
      "----- Epoch 22 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'secreted']\n",
      " Train loss: 90.6271979014\n",
      "0.15398915569326105\n",
      "----- Epoch 23 -----\n",
      "['mito' 'mito' 'mito' ..., 'mito' 'mito' 'mito']\n",
      " Train loss: 71.9958572388\n",
      "0.14794732765298219\n",
      "----- Epoch 24 -----\n",
      "['nucleus' 'nucleus' 'nucleus' ..., 'nucleus' 'nucleus' 'nucleus']\n",
      " Train loss: 66.0444242689\n",
      "0.3820294345468629\n",
      "----- Epoch 25 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 64.9625706143\n",
      "0.33044151820294343\n",
      "----- Epoch 26 -----\n",
      "['nucleus' 'nucleus' 'nucleus' ..., 'mito' 'nucleus' 'nucleus']\n",
      " Train loss: 70.3303829829\n",
      "0.37412858249419056\n",
      "----- Epoch 27 -----\n",
      "['nucleus' 'nucleus' 'nucleus' ..., 'nucleus' 'nucleus' 'nucleus']\n",
      " Train loss: 172.620276981\n",
      "0.3657629744384198\n",
      "----- Epoch 28 -----\n",
      "['nucleus' 'nucleus' 'nucleus' ..., 'nucleus' 'nucleus' 'nucleus']\n",
      " Train loss: 128.540040122\n",
      "0.36607281177381873\n",
      "----- Epoch 29 -----\n",
      "['cyto' 'cyto' 'cyto' ..., 'cyto' 'cyto' 'cyto']\n",
      " Train loss: 170.317114088\n",
      "0.330286599535244\n"
     ]
    }
   ],
   "source": [
    "### MODEL ###\n",
    "tf.reset_default_graph()\n",
    "n_hidden = 100\n",
    "shp = 80\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "## PLACEHOLDERS\n",
    "features_tf = tf.placeholder(tf.float32, [None,shp], \"feat\")       \n",
    "label_tf = tf.placeholder(tf.int32, [None,4], \"label\")             \n",
    "\n",
    "batch_size = tf.shape(features_tf)[0]\n",
    "\n",
    "\n",
    "### WEIGHTS AND BIASES ######\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.get_variable(name='wh1',shape=[shp, n_hidden]),\n",
    "    'out': tf.get_variable(name='whout',shape=[n_hidden, 4])\n",
    "}\n",
    "\n",
    "biases ={\n",
    "    'h1': tf.get_variable(name='bh1',shape=[1,n_hidden],initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'out': tf.get_variable(name='bout',shape=[1,4],initializer=tf.contrib.layers.xavier_initializer())\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Layers ######\n",
    "h1 = tf.nn.relu(tf.add(tf.matmul(features_tf,weights['h1']),biases['h1']))\n",
    "output = tf.add(tf.matmul(h1,weights['out']),biases['out'])  \n",
    "\n",
    "\n",
    "# loss \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, label_tf))\n",
    "                    \n",
    "\n",
    "#prediction function\n",
    "softmaxes = tf.nn.softmax(output)\n",
    "\n",
    "\n",
    "opt_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "BATCH_SIZE = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "    for epoch in range(30):\n",
    "        print('----- Epoch', epoch, '-----')\n",
    "        total_loss = 0\n",
    "        t = time.time()\n",
    "        n=18\n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            #labels_enc = label_encoder.fit_transform(train_y)\n",
    "            batch_y =label_encoder.fit_transform(train_y)\n",
    "            batch_x =vectorizer.fit_transform(feat_extract(train_x))\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            feed_dict ={features_tf:batch_x, label_tf:batch_y}\n",
    "            _, current_loss,predi = sess.run([opt_op, cost,softmaxes], feed_dict=feed_dict)\n",
    "            \n",
    "            total_loss += current_loss\n",
    "        predi = label_encoder.inverse_transform(predi)    \n",
    "        print(predi)\n",
    "        print(' Train loss:', total_loss / n)\n",
    "        count = 0\n",
    "        for i in range(0,len(predi)):\n",
    "            if predi[i] == train_y[i]:\n",
    "                count += 1\n",
    "        print(count/len(predi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-171-f34713db02e1>:51 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0001 cost= 98755.505642361\n",
      "Epoch: 0002 cost= 81222.852430556\n",
      "Epoch: 0003 cost= 63480.323567708\n",
      "Epoch: 0004 cost= 48108.561848958\n",
      "Epoch: 0005 cost= 37950.677517361\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 1\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 25 # 1st layer number of features\n",
    "n_input = 73 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 4 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "features_tf = tf.placeholder(\"float\", [None, n_input])\n",
    "label_tf = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(features_tf, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(features_tf, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(features_tf, weights, biases)\n",
    "soft_m = tf.nn.softmax(pred)\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=label_tf))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(18//batch_size):\n",
    "            batch_y =label_encoder.fit_transform(train_yy)\n",
    "            batch_x =vectorizer.fit_transform(feat_extract(train_xx))\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c,predi = sess.run([optimizer, cost,soft_m], feed_dict={features_tf: batch_x,\n",
    "                                                          label_tf: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / 18\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "            #print(predi)\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_y,pred_y)\n",
    "stats =  precision_recall_fscore_support(val_y,pred_y)\n",
    "\n",
    "stats = pd.DataFrame(data=np.transpose(np.array(stats[0:3])),columns=['precision','recall','f1'])\n",
    "print(cm)\n",
    "\n",
    "display(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ola = feat_extract(val_x)\n",
    "#ola[23]\n",
    "count = 0\n",
    "for i in range(0,len(val_y)):\n",
    "    if val_y[i] == pred_y[i]:\n",
    "        count += 1\n",
    "print(count/len(val_y))\n",
    "#len(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_blind(file,model):\n",
    "    f = open(file,'r')\n",
    "    preds = open('blind_predictions.txt','w')\n",
    "    sequence = ''\n",
    "    \n",
    "    first_line= f.readline()\n",
    "    first_line = first_line.rstrip('\\n')\n",
    "    preds.write(first_line + ' ')\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        line = line.rstrip('\\n')\n",
    "        if line[0] != '>':\n",
    "            sequence += line\n",
    "        else:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            feature = vectorizer.transform(feat_extract([sequence]))\n",
    "            predict = model.predict_proba(feature)\n",
    "            predict_label = np.argmax(predict,1)\n",
    "            label_predicted = label_encoder.inverse_transform(predict_label)\n",
    "            #preds.write(label_predicted[0] + ' \\t\\t' + str(np.amax(predict,1)[0]) + '\\n' + line + ' ')\n",
    "            preds.write(\"{0} {1:>8} \\n{2} \".format(label_predicted[0],str(np.amax(predict,1)[0]),line))\n",
    "            sequence = ''\n",
    "    feature = vectorizer.transform(feat_extract([sequence]))\n",
    "    predict = model.predict_proba(feature)\n",
    "    predict_label = np.argmax(predict,1)\n",
    "    label_predicted = label_encoder.inverse_transform(predict_label)\n",
    "    #preds.write(label_predicted[0] + ' \\t\\t' + str(np.amax(predict,1)[0]) + '\\n' + line + ' ')\n",
    "    preds.write(\"{0} {1}\".format(label_predicted[0],str(np.amax(predict,1)[0]),line))\n",
    "    sequence = ''\n",
    "    preds.close()\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_blind('blind.fasta',rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sequence, data_labels = preprocess_pipeline('cyto.fasta', 'mito.fasta','nucleus.fasta','secreted.fasta')\n",
    "train_x, val_x, train_y, val_y = train_test_split(data_sequence,data_labels,test_size=0.3,random_state=3)\n",
    "\n",
    "hist_train = Counter(train_y)\n",
    "hist_val = Counter(val_y)\n",
    "\n",
    "print(hist_train)\n",
    "print(hist_val)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = Counter([len(x) for x in data_sequence])\n",
    "#df = pd.DataFrame(hist,index=[0])\n",
    "#df = pd.DataFrame.from_dict(hist,orient='index')\n",
    "transposed = np.array(list(hist.items())).T\n",
    "x, y = transposed\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(feat_extract(train_x)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cyto.fasta') as fasta_file:  # Will close handle cleanly\n",
    "    identifiers = []\n",
    "    lengths = []\n",
    "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
    "        identifiers.append(seq_record.id)\n",
    "        lengths.append(len(seq_record.seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=re.compile(\"OS=([0-9a-zA-Z_\\s\\(\\)\\-\\/,\\.\\>\\:\\']+) (?:GN|PE)\")#\n",
    "meta_info = p.search('OS=Penicillium funiculosum PE=1 SV=1')\n",
    "meta_info.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "(\\w+\\|\\w+)\\|(\\w+\\s[0-90-9a-zA-Z_\\s\\(\\)\\-\\/]+)OS=([0-90-9a-zA-Z_\\s\\(\\)\\-\\/]+)GN=([0-90-9a-zA-Z_\\s-]+)PE=([0-9])+\\s[SV=]+([0-9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y[-4:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_input_10 to have shape (None, 76) but got array with shape (1, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0861ec138412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Joao Reis\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Joao Reis\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1252\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[0;32m   1253\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m                                    check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Joao Reis\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected dense_input_10 to have shape (None, 76) but got array with shape (1, 3)"
     ]
    }
   ],
   "source": [
    "nn.predict(np.array([[1,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = preprocessing.LabelEncoder().fit(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_yy = train_y\n",
    "train_xx = train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = vectorizer.transform(feat_extract(val_x))\n",
    "predicts = rf.predict_proba(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LabelBinarizer' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-d3f11c507c33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LabelBinarizer' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "label_encoder.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
